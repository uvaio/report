\section{Results}

\subsection{Website}

The original website has a lot of individual objects and detailed metadata about properties. The aim of the website was mainly to \textit{summarise} the collection to allow the before-mentioned users to explore the broad dataset and find interesting patterns.

The design is based on the branding of the original below the surface projects and follows standard information and visual design principles.

\subsubsection{Overview summary page}
Create a storytelling kind of page with 5 sections. HIER ENUMERATE ALLE SECTIES MET CHART TYPES

\subsubsection{Collection detail subset}
Based on the categories of the summary page a user can create a subset of the collection.

\subsection{Machine Learning Model}
Following training the neural network over 50 ephocs, the accuracy of the neural network was tested. An accuracy of aprox. 75\% was obtained over the test data, which the neural network has never seen before.The accuracy is considered satisfactory for the given application. Furthermore, the neural network was used to clasify the remaining unlabelled data, the results being deemed satisfactory.

\subsection{Dataset}

\subsubsection{Initial Dataset}
The initial dataset as provided in belowthesurface website was structured as a flat model with each row representing an individual archaeological object. This flat structure included many columns, each corresponding to specific attributes associated with the objects. These attributes stored a wide range of material categories, such as ceramics, building materials, fauna, glass, and more. While the flat model provided an accessible format for data entry, it led to redundant storage of certain information, challenging in visualization, particularly common attributes that applied to all finds.

\subsubsection{Data Transformation and Normalization}
As the dataset was provided in a flat model, which presented certain challenges in terms of data redundancy and efficient visualization. To address these issues and enhance the integrity of the data, a normalization process was done to fit the data into the relational model as described in the Data Model section.

\subsubsection{Normalized datasets}
All the datasets in accordance with the datamodel is uploaded into the datasets repository of GitHub. A csv file for each table of the data model is created based on the initial dataset.

\subsection{Ontology and Data Dictionary}
The initial data dictionary provided by the below the surface website contained limited descriptions of the metadata. This posed a challenge to understanding the information in the flatfile dataset.  Further work was performed to enrich the information so that it would be useful.  For example, adding information in about which geographic locations referred to which project code.

The ontology was created from scratch as none was supplied with the original website.  Over 35 classes were created which covered aspects of the find-number, location, and various physical characteristics of the objects.  It is now possible to determine which project code refers to which physical location.  All of the finds were imported as individuals and assigned as a type of 'vondstnummer' or 'find'.  To import the find numbers into a turtle format correctly, the full stops needed to be removed from the find numbers.  Having the full stop included resulted in the turtle file becoming unreadable. The code to assign the find number to the correct class in the ontology is provided.    
